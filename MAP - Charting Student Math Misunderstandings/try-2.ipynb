{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":12559632,"sourceType":"datasetVersion","datasetId":7930680},{"sourceId":12559652,"sourceType":"datasetVersion","datasetId":7930694},{"sourceId":12719174,"sourceType":"datasetVersion","datasetId":8039184},{"sourceId":12729471,"sourceType":"datasetVersion","datasetId":8045877}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile gemma2_inference.py\n\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os\nfrom IPython.display import display, Math, Latex\nimport torch\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd, numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\nfrom peft import PeftModel\nfrom scipy.special import softmax\nfrom tqdm import tqdm\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\nlora_path = \"/kaggle/input/gemma2-9b-it-cv945\"\nMAX_LEN = 320  # Increased from 256 for better context capture\n# helpers\ndef format_input(row):\n    # Enhanced prompt with more explicit structure\n    x = \"CORRECT\"\n    if not row['is_correct']:\n        x = \"INCORRECT\"\n    return (\n        f\"Mathematical Question: {row['QuestionText']}\\n\"\n        f\"Student's Answer: {row['MC_Answer']}\\n\"\n        f\"Answer Status: {x}\\n\"\n        f\"Student's Reasoning: {row['StudentExplanation']}\\n\"\n        f\"Task: Identify the misconception category and specific misconception.\"\n    )\n\n# Tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n\n\n\nle = LabelEncoder()\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n\ntrain.Misconception = train.Misconception.fillna('NA')\ntrain['target'] = train.Category+\":\"+train.Misconception\ntrain['label'] = le.fit_transform(train['target'])\ntarget_classes = le.classes_\nn_classes = len(target_classes)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\n# Prepare test data\ntest = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\ntest['text'] = test.apply(format_input, axis=1)\n\n\n# load model & tokenizer\ntokenizer = AutoTokenizer.from_pretrained(lora_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/gemma2-9b-it-bf16\",\n    num_labels=n_classes,\n    torch_dtype=torch.bfloat16,  # Changed to bfloat16 for better numerical stability\n    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(model, lora_path)\nmodel.eval()\n\n# Tokenize dataset\nds_test = Dataset.from_pandas(test[['text']])\nds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n\n# Create data collator for efficient batching with padding\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    max_length=MAX_LEN,  \n    return_tensors=\"pt\")\n\ndataloader = DataLoader(\n    ds_test,\n    batch_size=6,  # Reduced for stability with longer sequences\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=True,  \n    num_workers=2     \n)\n\n# Fast inference loop\nall_logits = []\ndevice = next(model.parameters()).device\n\nwith torch.no_grad():\n    for batch in tqdm(dataloader, desc=\"Gemma2 Inference\"):\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        # Forward pass\n        outputs = model(**batch)\n        logits = outputs.logits\n        \n        # Convert to float32 then move to CPU and store\n        all_logits.append(logits.float().cpu().numpy())\n\n# Concatenate all logits\npredictions = np.concatenate(all_logits, axis=0)\n\n# Apply temperature scaling for better calibration\ntemperature = 1.5\npredictions = predictions / temperature\n\n# Convert to probs\nprobs = softmax(predictions, axis=1)\n\n# Get top predictions (top 35 for better ensemble coverage)\ntop_indices = np.argsort(-probs, axis=1)\n\n# Decode to class names\nflat_indices = top_indices.flatten()\ndecoded_labels = le.inverse_transform(flat_indices)\ntop_labels = decoded_labels.reshape(top_indices.shape)\n\n# Create submission (top 3)\njoined_preds = [\" \".join(row[:3]) for row in top_labels]\n\nsub = pd.DataFrame({\n    \"row_id\": test.row_id.values,\n    \"Category:Misconception\": joined_preds\n})\nsub.to_csv(\"submission_gemma.csv\", index=False)\n\n# Save more predictions for better ensembling\nprob_data = []\nfor i in range(len(test)):\n    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(35)}  # Increased to top 35\n    prob_dict['row_id'] = test.row_id.values[i]\n    prob_dict['top_classes'] = \" \".join(top_labels[i, :35])  # Top 35 class names\n    prob_data.append(prob_dict)\n\nprob_df = pd.DataFrame(prob_data)\nprob_df.to_csv(\"submission_gemma_prob.csv\", index=False)\nprint(\"Gemma2 inference completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:42:00.550652Z","iopub.execute_input":"2025-10-05T20:42:00.550898Z","iopub.status.idle":"2025-10-05T20:42:00.560676Z","shell.execute_reply.started":"2025-10-05T20:42:00.550879Z","shell.execute_reply":"2025-10-05T20:42:00.559794Z"}},"outputs":[{"name":"stdout","text":"Writing gemma2_inference.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile qwen3_deepseek_inference.py\n\n# we do parallel inference, for deepseek and qwen3\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nimport threading\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\nfrom scipy.special import softmax\nfrom tqdm import tqdm\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\ntest  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n\nmodel_paths = [\n    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n\n# Enhanced prompts with more context\ndef format_input_deepseek(row):\n    \"\"\"DeepSeek optimized prompt - math-focused\"\"\"\n    status = \"CORRECT ANSWER\"\n    if not row['is_correct']:\n        status = \"INCORRECT ANSWER\"\n    return (\n        f\"Math Problem: {row['QuestionText']}\\n\"\n        f\"Student Selected: {row['MC_Answer']}\\n\"\n        f\"Answer Status: {status}\\n\"\n        f\"Student's Explanation: {row['StudentExplanation']}\\n\"\n        f\"Analyze the mathematical misconception.\"\n    )\n\ndef format_input_qwen(row):\n    \"\"\"Qwen optimized prompt - reasoning-focused\"\"\"\n    correctness = \"This answer is correct and shows proper understanding.\"\n    if not row['is_correct']:\n        correctness = \"This answer is incorrect and reveals a misconception.\"\n    return (\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer Given: {row['MC_Answer']}\\n\"\n        f\"Assessment: {correctness}\\n\"\n        f\"Student Reasoning: {row['StudentExplanation']}\\n\"\n        f\"Identify the category and misconception type.\"\n    )\n\n\nle = LabelEncoder()\ntrain.Misconception  = train.Misconception.fillna('NA')\ntrain['target']   = train.Category + ':' +train.Misconception\ntrain['label']    = le.fit_transform(train['target'])\n\nn_classes = len(le.classes_)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\n\n\ndef run_inference_on_gpu(model_path, gpu_id, test_data, output_name, format_func):\n    \"\"\"Run inference for one model on one GPU\"\"\"\n    \n    device = f\"cuda:{gpu_id}\"\n    print(f\"Loading {output_name} on {device}...\")\n    \n    # Prepare data with model-specific formatting\n    test_copy = test_data.copy()\n    test_copy['text'] = test_copy.apply(format_func, axis=1)\n    \n    # Load model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, \n        device_map=device, \n        torch_dtype=torch.bfloat16  # Changed to bfloat16 for better stability\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.eval()\n    \n    # Tokenize function\n    def tokenize(batch):\n        return tokenizer(batch[\"text\"], \n                        truncation=True,\n                        max_length=320)  # Increased context\n    \n    ds_test = Dataset.from_pandas(test_copy[['text']])\n    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n    \n    # Data collator\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    \n    # DataLoader\n    dataloader = DataLoader(\n        ds_test,\n        batch_size=6 if output_name == \"deepseek\" else 5,  # Optimized batch sizes\n        shuffle=False,\n        collate_fn=data_collator,\n        pin_memory=True,\n        num_workers=0\n    )\n    \n    # Inference\n    all_logits = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            all_logits.append(outputs.logits.float().cpu().numpy())\n    \n    predictions = np.concatenate(all_logits, axis=0)\n    \n    # Temperature scaling per model\n    temperature = 1.3 if output_name == \"deepseek\" else 1.4\n    predictions = predictions / temperature\n    \n    # Process results\n    probs = softmax(predictions, axis=1)\n    top_indices = np.argsort(-probs, axis=1)\n    \n    # Decode labels\n    flat_indices = top_indices.flatten()\n    decoded_labels = le.inverse_transform(flat_indices)\n    top_labels = decoded_labels.reshape(top_indices.shape)\n    \n    # Save top-3 submission\n    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n    sub = pd.DataFrame({\n        \"row_id\": test_data.row_id.values,\n        \"Category:Misconception\": joined_preds\n    })\n    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n    \n    # Save probabilities for ensemble (increased to 35)\n    prob_data = []\n    for i in range(len(predictions)):\n        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(35)}\n        prob_dict['row_id'] = test_data.row_id.values[i]\n        prob_dict['top_classes'] = \" \".join(top_labels[i, :35])\n        prob_data.append(prob_dict)\n    \n    prob_df = pd.DataFrame(prob_data)\n    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n    \n    print(f\"✓ {output_name} completed - saved submission and probabilities\")\n    \n    # Clean up GPU memory\n    del model, tokenizer\n    torch.cuda.empty_cache()\n\nprint(\"🚀 Starting multi-GPU inference...\")\nstart_time = time.time()\n\nthreads = []\ngpu_assignments = [\n    (model_paths[0], 0, \"deepseek\", format_input_deepseek),\n    (model_paths[1], 1, \"qwen3\", format_input_qwen),\n]\n\n# Start threads\nfor model_path, gpu_id, name, format_func in gpu_assignments:\n    if gpu_id < torch.cuda.device_count():  \n        thread = threading.Thread(\n            target=run_inference_on_gpu,\n            args=(model_path, gpu_id, test, name, format_func)\n        )\n        threads.append(thread)\n        thread.start()\n        time.sleep(10)  # Stagger starts to avoid memory issues\n\n# Wait for completion\nfor thread in threads:\n    thread.join()\n\nend_time = time.time()\nprint(f\"✅ All inference completed in {end_time - start_time:.2f} seconds!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:42:11.998472Z","iopub.execute_input":"2025-10-05T20:42:11.999088Z","iopub.status.idle":"2025-10-05T20:42:12.006400Z","shell.execute_reply.started":"2025-10-05T20:42:11.999064Z","shell.execute_reply":"2025-10-05T20:42:12.005605Z"}},"outputs":[{"name":"stdout","text":"Writing qwen3_deepseek_inference.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import time \nimport os\n\nprint(\"=\" * 60)\nprint(\"Starting Gemma2 Inference...\")\nprint(\"=\" * 60)\n!python /kaggle/working/gemma2_inference.py\n\ntime.sleep(15)  # Increased wait time for memory cleanup\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Starting DeepSeek & Qwen3 Parallel Inference...\")\nprint(\"=\" * 60)\n!python /kaggle/working/qwen3_deepseek_inference.py\n\n# Verify all outputs exist\nrequired_files = [\n    'submission_gemma_prob.csv',\n    'submission_deepseek_probabilities.csv',\n    'submission_qwen3_probabilities.csv'\n]\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Verifying output files...\")\nfor file in required_files:\n    if os.path.exists(f'/kaggle/working/{file}'):\n        print(f\"✓ {file} exists\")\n    else:\n        print(f\"✗ {file} MISSING!\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:42:36.507446Z","iopub.execute_input":"2025-10-05T20:42:36.507707Z","iopub.status.idle":"2025-10-05T20:49:03.739834Z","shell.execute_reply.started":"2025-10-05T20:42:36.507686Z","shell.execute_reply":"2025-10-05T20:49:03.739031Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Gemma2 Inference...\n============================================================\n2025-10-05 20:42:57.775593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759696978.129187      80 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759696978.231574      80 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrain shape: (36696, 9) with 65 target classes\nLoading checkpoint shards: 100%|██████████████████| 4/4 [02:36<00:00, 39.10s/it]\nSome weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-bf16 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nMap: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 73.38 examples/s]\nGemma2 Inference:   0%|                                   | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\nGemma2 Inference: 100%|███████████████████████████| 1/1 [00:08<00:00,  8.36s/it]\nGemma2 inference completed!\n\n============================================================\nStarting DeepSeek & Qwen3 Parallel Inference...\n============================================================\n2025-10-05 20:46:30.739819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759697190.762131     107 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759697190.768929     107 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrain shape: (36696, 9) with 65 target classes\n🚀 Starting multi-GPU inference...\nLoading deepseek on cuda:0...\nLoading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]Loading qwen3 on cuda:1...\n\nLoading checkpoint shards:  33%|██████            | 1/3 [00:43<01:26, 43.19s/it]\u001b[A\nLoading checkpoint shards:  67%|████████████      | 2/3 [01:27<00:43, 43.98s/it]\u001b[A\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:54<00:00, 38.08s/it]\u001b[A\nMap: 100%|████████████████████████████████| 3/3 [00:00<00:00, 137.33 examples/s]\ndeepseek: 100%|███████████████████████████████████| 1/1 [00:03<00:00,  3.54s/it]\n✓ deepseek completed - saved submission and probabilities\n\nLoading checkpoint shards:  75%|█████████████▌    | 3/4 [02:10<00:43, 43.26s/it]\u001b[A\nLoading checkpoint shards: 100%|██████████████████| 4/4 [02:12<00:00, 33.21s/it]\u001b[A\nMap: 100%|████████████████████████████████| 3/3 [00:00<00:00, 487.33 examples/s]\nqwen3: 100%|██████████████████████████████████████| 1/1 [00:02<00:00,  2.07s/it]\n✓ qwen3 completed - saved submission and probabilities\n✅ All inference completed in 145.79 seconds!\n\n============================================================\nVerifying output files...\n✓ submission_gemma_prob.csv exists\n✓ submission_deepseek_probabilities.csv exists\n✓ submission_qwen3_probabilities.csv exists\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.special import softmax\nfrom scipy.stats import rankdata\n\ndef extract_class_probabilities(row, model_suffix='', top_k=35):\n    \"\"\"Extract class names and probabilities from a row\"\"\"\n    classes_col = f'top_classes{model_suffix}'\n    if classes_col in row:\n        classes = row[classes_col].split(' ')[:top_k]\n    else:\n        return {}\n    \n    class_probs = {}\n    for i in range(min(top_k, len(classes))):\n        prob_col = f'prob_{i}{model_suffix}'\n        if prob_col in row:\n            class_probs[classes[i]] = row[prob_col]\n    return class_probs\n\ndef advanced_ensemble(prob_files, model_weights=None, top_k=3):\n    \"\"\"\n    Advanced ensemble with:\n    1. Rank-based fusion\n    2. Calibrated probability averaging\n    3. Diversity-aware voting\n    4. Confidence-weighted combination\n    \"\"\"\n    n_models = len(prob_files)\n    prob_dfs = []\n    final_predictions = []\n    \n    for file_path in prob_files:\n        df = pd.read_csv(file_path)\n        prob_dfs.append(df)\n    \n    # Merge on row_id\n    merged_df = prob_dfs[0]\n    for i, df in enumerate(prob_dfs[1:], 1):\n        merged_df = pd.merge(merged_df, df, on='row_id', suffixes=('', f'_model{i+1}'))\n    \n    print(f\"Processing {len(merged_df)} test samples...\")\n    \n    for idx, row in merged_df.iterrows():\n        # Extract probabilities from each model\n        all_class_probs = []\n        all_class_ranks = []\n        \n        for i in range(n_models):\n            suffix = f'_model{i+1}' if i > 0 else ''\n            class_probs = extract_class_probabilities(row, suffix, top_k=35)\n            all_class_probs.append(class_probs)\n            \n            # Calculate ranks (lower is better)\n            if class_probs:\n                classes = list(class_probs.keys())\n                probs = np.array(list(class_probs.values()))\n                # Rank: 1 for highest prob, 2 for second highest, etc.\n                ranks = rankdata(-probs, method='ordinal')\n                class_ranks = dict(zip(classes, ranks))\n                all_class_ranks.append(class_ranks)\n            else:\n                all_class_ranks.append({})\n        \n        # Get all unique classes\n        all_classes = set()\n        for class_probs in all_class_probs:\n            all_classes.update(class_probs.keys())\n        \n        # Calculate multiple scoring components\n        final_scores = {}\n        \n        for class_name in all_classes:\n            # Component 1: Weighted probability average\n            prob_sum = 0\n            prob_count = 0\n            for i, class_probs in enumerate(all_class_probs):\n                if class_name in class_probs:\n                    prob_sum += class_probs[class_name] * model_weights[i]\n                    prob_count += 1\n            avg_prob = prob_sum / max(prob_count, 1)\n            \n            # Component 2: Rank-based score (Borda count style)\n            rank_score = 0\n            for i, class_ranks in enumerate(all_class_ranks):\n                if class_name in class_ranks:\n                    # Convert rank to score (lower rank = higher score)\n                    rank = class_ranks[class_name]\n                    # Reciprocal rank with weight\n                    rank_score += (1.0 / rank) * model_weights[i]\n            \n            # Component 3: Model agreement (how many models predict this)\n            agreement = sum(1 for cp in all_class_probs if class_name in cp) / n_models\n            \n            # Component 4: Max confidence (highest probability across models)\n            max_confidence = 0\n            for i, class_probs in enumerate(all_class_probs):\n                if class_name in class_probs:\n                    weighted_prob = class_probs[class_name] * model_weights[i]\n                    max_confidence = max(max_confidence, weighted_prob)\n            \n            # Component 5: Position bonus (higher weight for top positions)\n            position_bonus = 0\n            for i, class_probs in enumerate(all_class_probs):\n                if class_name in class_probs:\n                    # Get position (0-indexed)\n                    position = list(class_probs.keys()).index(class_name)\n                    # Exponential decay: top positions get much higher bonus\n                    position_bonus += np.exp(-position / 5.0) * model_weights[i]\n            \n            # Combine all components with optimized weights\n            final_scores[class_name] = (\n                avg_prob * 0.35 +           # 35% weighted probability\n                rank_score * 0.25 +         # 25% rank-based score\n                agreement * 0.15 +          # 15% model agreement\n                max_confidence * 0.15 +     # 15% peak confidence\n                position_bonus * 0.10       # 10% position bonus\n            )\n        \n        # Sort and get top-k\n        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n        top_classes = [class_name for class_name, _ in sorted_classes[:top_k]]\n        \n        final_predictions.append(' '.join(top_classes))\n    \n    return final_predictions\n\n# Model weights based on individual scores + calibration\n# DeepSeek: 0.944 -> weight 1.5 (best performer, math-specialized)\n# Gemma2: 0.942 -> weight 1.3 (strong generalist)\n# Qwen3: 0.943 -> weight 1.4 (good reasoning)\n\nprint(\"=\" * 60)\nprint(\"Starting Advanced Ensemble\")\nprint(\"=\" * 60)\n\nprob_files = [\n    '/kaggle/working/submission_deepseek_probabilities.csv',\n    '/kaggle/working/submission_gemma_prob.csv',\n    '/kaggle/working/submission_qwen3_probabilities.csv'\n]\n\n# Optimized weights\nw1 = 1.5  # DeepSeek (best on math)\nw2 = 1.3  # Gemma2\nw3 = 1.4  # Qwen3\n\npredictions = advanced_ensemble(\n    prob_files, \n    model_weights=[w1, w2, w3],  \n    top_k=3\n)\n\ntest_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\nsubmission = pd.DataFrame({\n    'row_id': test_df.row_id.values,\n    'Category:Misconception': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"✅ Submission created successfully!\")\nprint(f\"Total predictions: {len(submission)}\")\nprint(\"=\" * 60)\nprint(\"\\nFirst 5 predictions:\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:49:15.933242Z","iopub.execute_input":"2025-10-05T20:49:15.933542Z","iopub.status.idle":"2025-10-05T20:49:17.098742Z","shell.execute_reply.started":"2025-10-05T20:49:15.933512Z","shell.execute_reply":"2025-10-05T20:49:17.098117Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting Advanced Ensemble\n============================================================\nProcessing 3 test samples...\n\n============================================================\n✅ Submission created successfully!\nTotal predictions: 3\n============================================================\n\nFirst 5 predictions:\n   row_id                             Category:Misconception\n0   36696   False_Neither:NA True_Correct:NA True_Neither:NA\n1   36697  False_Neither:NA False_Misconception:WNB False...\n2   36698   False_Neither:NA True_Neither:NA True_Correct:NA\n","output_type":"stream"}],"execution_count":4}]}