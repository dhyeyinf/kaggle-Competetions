{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12559632,"sourceType":"datasetVersion","datasetId":7930680},{"sourceId":12559652,"sourceType":"datasetVersion","datasetId":7930694},{"sourceId":12719174,"sourceType":"datasetVersion","datasetId":8039184},{"sourceId":12729471,"sourceType":"datasetVersion","datasetId":8045877}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile gemma2_inference.py\n\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os\nfrom IPython.display import display, Math, Latex\nimport torch\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd, numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\nfrom peft import PeftModel\nfrom scipy.special import softmax\nfrom tqdm import tqdm\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\nlora_path = \"/kaggle/input/gemma2-9b-it-cv945\"\nMAX_LEN = 256\n# helpers\ndef format_input(row):\n    x = \"Yes\"\n    if not row['is_correct']:\n        x = \"No\"\n    return (\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct? {x}\\n\"\n        f\"Student Explanation: {row['StudentExplanation']}\"\n    )\n\n# Tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\n\n\nle = LabelEncoder()\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n\ntrain.Misconception = train.Misconception.fillna('NA')\ntrain['target'] = train.Category+\":\"+train.Misconception\ntrain['label'] = le.fit_transform(train['target'])\ntarget_classes = le.classes_\nn_classes = len(target_classes)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\n# Prepare test data\ntest = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\ntest['text'] = test.apply(format_input, axis=1)\n\n\n# load model & tokenizer\ntokenizer = AutoTokenizer.from_pretrained(lora_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/gemma2-9b-it-bf16\",\n    num_labels=n_classes,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(model, lora_path)\nmodel.eval()\n\n# Tokenize dataset\nds_test = Dataset.from_pandas(test[['text']])\nds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n\n# Create data collator for efficient batching with padding\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    max_length=MAX_LEN,  \n    return_tensors=\"pt\")\n\ndataloader = DataLoader(\n    ds_test,\n    batch_size=8,  \n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=True,  \n    num_workers=2     \n)\n\n# Fast inference loop\nall_logits = []\ndevice = next(model.parameters()).device\n\nwith torch.no_grad():\n    for batch in tqdm(dataloader, desc=\"Inference\"):\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        # Forward pass\n        outputs = model(**batch)\n        logits = outputs.logits\n        \n        # Convert bfloat16 to float32 then move to CPU and store\n        all_logits.append(logits.float().cpu().numpy())\n\n# Concatenate all logits\npredictions = np.concatenate(all_logits, axis=0)\n\n# Convert to probs\nprobs = softmax(predictions, axis=1)\n\n# Get top predictions (all 65 classes ranked)\ntop_indices = np.argsort(-probs, axis=1)\n\n# Decode to class names\nflat_indices = top_indices.flatten()\ndecoded_labels = le.inverse_transform(flat_indices)\ntop_labels = decoded_labels.reshape(top_indices.shape)\n\n# Create submission (top 3)\njoined_preds = [\" \".join(row[:3]) for row in top_labels]\n\nsub = pd.DataFrame({\n    \"row_id\": test.row_id.values,\n    \"Category:Misconception\": joined_preds\n})\nsub.to_csv(\"submission_gemma.csv\", index=False)\n\nprob_data = []\nfor i in range(len(test)):\n    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}  # Top 25\n    prob_dict['row_id'] = test.row_id.values[i]\n    prob_dict['top_classes'] = \" \".join(top_labels[i, :25])  # Top 25 class names\n    prob_data.append(prob_dict)\n\nprob_df = pd.DataFrame(prob_data)\nprob_df.to_csv(\"submission_gemma_prob.csv\", index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:45:41.719548Z","iopub.execute_input":"2025-10-02T20:45:41.719778Z","iopub.status.idle":"2025-10-02T20:45:41.729134Z","shell.execute_reply.started":"2025-10-02T20:45:41.719751Z","shell.execute_reply":"2025-10-02T20:45:41.728355Z"}},"outputs":[{"name":"stdout","text":"Writing gemma2_inference.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile qwen3_deepseek_inference.py\n\n# we do parallel inference, for deepseek and qwen3\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nimport threading\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\nfrom scipy.special import softmax\nfrom tqdm import tqdm\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\ntest  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n\nmodel_paths = [\n    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n\ndef format_input(row):\n    x = \"This answer is correct.\"\n    if not row['is_correct']:\n        x = \"This is answer is incorrect.\"\n    return (\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"{x}\\n\"\n        f\"Student Explanation: {row['StudentExplanation']}\")\n\n\nle = LabelEncoder()\ntrain.Misconception  = train.Misconception.fillna('NA')\ntrain['target']   = train.Category + ':' +train.Misconception\ntrain['label']    = le.fit_transform(train['target'])\n\nn_classes = len(le.classes_)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\ntest['text'] = test.apply(format_input,axis=1)\nds_test = Dataset.from_pandas(test)\n\n\ndef run_inference_on_gpu(model_path, gpu_id, test_data, output_name):\n    \"\"\"Run inference for one model on one GPU\"\"\"\n    \n    device = f\"cuda:{gpu_id}\"\n    print(f\"Loading {output_name} on {device}...\")\n    \n    # Load model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, \n        device_map=device, \n        torch_dtype=torch.float16\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.eval()\n    \n    # Tokenize function\n    def tokenize(batch):\n        return tokenizer(batch[\"text\"], \n                        truncation=True,\n                        max_length=256)\n    \n    ds_test = Dataset.from_pandas(test_data[['text']])\n    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n    \n    # Data collator\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n    \n    # DataLoader\n    dataloader = DataLoader(\n        ds_test,\n        batch_size=4,\n        shuffle=False,\n        collate_fn=data_collator,\n        pin_memory=True,\n        num_workers=0\n    )\n    \n    # Inference\n    all_logits = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            all_logits.append(outputs.logits.float().cpu().numpy())\n    \n    predictions = np.concatenate(all_logits, axis=0)\n    \n    # Process results\n    probs = softmax(predictions, axis=1)\n    top_indices = np.argsort(-probs, axis=1)\n    \n    # Decode labels\n    flat_indices = top_indices.flatten()\n    decoded_labels = le.inverse_transform(flat_indices)\n    top_labels = decoded_labels.reshape(top_indices.shape)\n    \n    # Save top-3 submission\n    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n    sub = pd.DataFrame({\n        \"row_id\": test_data.row_id.values,\n        \"Category:Misconception\": joined_preds\n    })\n    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n    \n    # Save probabilities for ensemble\n    prob_data = []\n    for i in range(len(predictions)):\n        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n        prob_dict['row_id'] = test_data.row_id.values[i]\n        prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n        prob_data.append(prob_dict)\n    \n    prob_df = pd.DataFrame(prob_data)\n    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n    \n    print(f\" {output_name} completed - saved submission and probabilities\")\n    \n    # Clean up GPU memory\n    del model, tokenizer\n    torch.cuda.empty_cache()\n\nprint(\" Starting multi-GPU inference...\")\nstart_time = time.time()\n\nthreads = []\ngpu_assignments = [\n    (model_paths[0], 0, \"deepseek\"),\n    (model_paths[1], 1, \"qwen3\"),\n]\n\n# Start threads\nfor model_path, gpu_id, name in gpu_assignments:\n    if gpu_id < torch.cuda.device_count():  \n        thread = threading.Thread(\n            target=run_inference_on_gpu,\n            args=(model_path, gpu_id, test, name)\n        )\n        threads.append(thread)\n        thread.start()\n        time.sleep(10)  # Stagger starts to avoid memory issues\n\n# Wait for completion\nfor thread in threads:\n    thread.join()\n\nend_time = time.time()\nprint(f\" completed in {end_time - start_time:.2f} seconds!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:46:16.394888Z","iopub.execute_input":"2025-10-02T20:46:16.395661Z","iopub.status.idle":"2025-10-02T20:46:16.402880Z","shell.execute_reply.started":"2025-10-02T20:46:16.395634Z","shell.execute_reply":"2025-10-02T20:46:16.402122Z"}},"outputs":[{"name":"stdout","text":"Overwriting qwen3_deepseek_inference.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import time \n!python /kaggle/working/gemma2_inference.py\ntime.sleep(10)\n!python /kaggle/working/qwen3_deepseek_inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:46:30.938758Z","iopub.execute_input":"2025-10-02T20:46:30.939460Z","iopub.status.idle":"2025-10-02T20:51:48.460712Z","shell.execute_reply.started":"2025-10-02T20:46:30.939432Z","shell.execute_reply":"2025-10-02T20:51:48.459998Z"}},"outputs":[{"name":"stdout","text":"2025-10-02 20:46:52.304625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759438012.655686      78 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759438012.748399      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrain shape: (36696, 9) with 65 target classes\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:36<00:00, 24.08s/it]\nSome weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-bf16 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\nMap: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 79.01 examples/s]\nInference:   0%|                                          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\nInference: 100%|██████████████████████████████████| 1/1 [00:02<00:00,  2.90s/it]\n2025-10-02 20:49:15.059620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759438155.081473     105 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759438155.088353     105 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrain shape: (36696, 9) with 65 target classes\n Starting multi-GPU inference...\nLoading deepseek on cuda:0...\nLoading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]Loading qwen3 on cuda:1...\n\nLoading checkpoint shards:  33%|██████            | 1/3 [00:45<01:31, 45.61s/it]\u001b[A\nLoading checkpoint shards:  67%|████████████      | 2/3 [01:29<00:44, 44.87s/it]\u001b[A\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:57<00:00, 39.15s/it]\u001b[A\nMap: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 61.02 examples/s]\ndeepseek: 100%|███████████████████████████████████| 1/1 [00:02<00:00,  2.47s/it]\n deepseek completed - saved submission and probabilities\n\nLoading checkpoint shards:  75%|█████████████▌    | 3/4 [02:12<00:44, 44.25s/it]\u001b[A\nLoading checkpoint shards: 100%|██████████████████| 4/4 [02:15<00:00, 33.89s/it]\u001b[A\nMap: 100%|████████████████████████████████| 3/3 [00:00<00:00, 436.63 examples/s]\nqwen3: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  3.01it/s]\n qwen3 completed - saved submission and probabilities\n completed in 146.74 seconds!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.special import softmax\n\n\n\ndef extract_class_probabilities(row, model_suffix='', top_k=25):\n    \"\"\"Extract class names and probabilities from a row\"\"\"\n    # Get top classes\n    classes_col = f'top_classes{model_suffix}'\n    if classes_col in row:\n        classes = row[classes_col].split(' ')[:top_k]\n    else:\n        return {}\n    # Get probabilities\n    class_probs = {}\n    for i in range(min(top_k, len(classes))):\n        prob_col = f'prob_{i}{model_suffix}'\n        if prob_col in row:\n            class_probs[classes[i]] = row[prob_col]\n    return class_probs\n\n\ndef ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3):\n    n_models = len(prob_files)\n    prob_dfs = []\n    final_predictions = []\n    \n    for file_path in prob_files:\n        df = pd.read_csv(file_path)\n        prob_dfs.append(df)\n    \n    # Merge on row_id\n    merged_df = prob_dfs[0]\n    for i, df in enumerate(prob_dfs[1:], 1):\n        merged_df = pd.merge(merged_df, df, on='row_id', suffixes=('', f'_model{i+1}'))\n      \n    for idx, row in merged_df.iterrows():\n        \n        # Extract probabilities from each model\n        all_class_probs = []\n        for i in range(n_models):\n            suffix = f'_model{i+1}' if i > 0 else ''\n            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n            all_class_probs.append(class_probs)\n        \n        # Get all unique classes\n        all_classes = set()\n        for class_probs in all_class_probs:\n            all_classes.update(class_probs.keys())\n        \n        # Calculate agreement and disagreement\n        class_votes = defaultdict(int)\n        class_total_prob = defaultdict(float)\n        class_max_prob = defaultdict(float)\n        \n        for i, class_probs in enumerate(all_class_probs):\n            weight = model_weights[i]\n            \n            for class_name, prob in class_probs.items():\n                class_votes[class_name] += 1\n                class_total_prob[class_name] += prob * weight\n                class_max_prob[class_name] = max(class_max_prob[class_name], prob * weight)\n        \n        final_scores = {}\n        for class_name in all_classes:\n            \n            # Base score: weighted average probability\n            base_score = class_total_prob[class_name]\n            \n            # Agreement : classes predicted by more models get boost\n            agreement_bonus = class_votes[class_name] / n_models\n            \n            # Confidence bonus: classes with high max probability get boost\n            confidence_bonus = class_max_prob[class_name]\n            \n            # Combined score\n            final_scores[class_name] = (\n                base_score * 0.6 +           # 60% base probs\n                agreement_bonus * 0.3 +      # 30% agreement\n                confidence_bonus * 0.1       # 10% confidence\n            )\n        \n        # Sort and get top-k\n        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n        top_classes = [class_name for class_name, _ in sorted_classes[:top_k]]\n        \n        final_predictions.append(' '.join(top_classes))\n    \n    return final_predictions\n\n# single models scores\n# deepseek math 7b - 0.944\n# qwen3 8b - 0.943\n# gemma 2 9b - 0.942\nw1 = 1.2\nw2 = 1.0\nw3 = 0.8\n\nprob_files = [\n    '/kaggle/working/submission_deepseek_probabilities.csv',\n    '/kaggle/working/submission_gemma_prob.csv',\n        '/kaggle/working/submission_qwen3_probabilities.csv'\n\n]\n\n\npredictions = ensemble_with_disagreement_handling(\n        prob_files, \n        model_weights=[w1, w2, w3],  \n        top_k=3\n    )\n    \ntest_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n\nsubmission = pd.DataFrame({\n    'row_id': test_df.row_id.values,\n    'Category:Misconception': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T21:13:35.054015Z","iopub.execute_input":"2025-10-02T21:13:35.054788Z","iopub.status.idle":"2025-10-02T21:13:35.994530Z","shell.execute_reply.started":"2025-10-02T21:13:35.054755Z","shell.execute_reply":"2025-10-02T21:13:35.993957Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   row_id                             Category:Misconception\n0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n1   36697  False_Misconception:WNB False_Neither:NA False...\n2   36698  True_Neither:NA True_Correct:NA True_Misconcep...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>Category:Misconception</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>36696</td>\n      <td>True_Correct:NA True_Neither:NA True_Misconcep...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>36697</td>\n      <td>False_Misconception:WNB False_Neither:NA False...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36698</td>\n      <td>True_Neither:NA True_Correct:NA True_Misconcep...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5}]}
